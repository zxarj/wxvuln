# -*- coding: utf-8 -*-
import os
import re
import sys
import json
import xml.etree.ElementTree as ET
import platform
import tempfile
import requests
import shutil
import subprocess
import datetime
import argparse
import glob
import calendar
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('run.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


def write_json(path, data, encoding="utf8"):
    """å†™å…¥json"""
    with open(path, "w", encoding=encoding) as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


def read_json(path, default_data={}, encoding="utf8"):
    """è¯»å–json"""
    data = {}
    if os.path.exists(path):
        try:
            data = json.loads(open(path, "r", encoding=encoding).read())
        except:
            data = default_data
            write_json(path, data, encoding=encoding)

    else:
        data = default_data
        write_json(path, data, encoding=encoding)
    return data

def get_executable_path():
    '''è·å–å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„'''
    system = platform.system()
    if system == 'Windows':
        executable_path = './bin/wechatmp2markdown-v1.1.11_win64.exe'
    else:
        executable_path = './bin/wechatmp2markdown-v1.1.11_linux_amd64'
    # æ·»åŠ æ‰§è¡Œæƒé™
    os.chmod(executable_path, 0o755)
    # è¿”å›å¯æ‰§è¡Œæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    return executable_path

def get_md_path(executable_path,url):
    '''è·å–mdæ–‡ä»¶è·¯å¾„'''
    temp_directory = tempfile.mkdtemp()
    command = [executable_path, url, temp_directory, '--image=url']
    subprocess.check_output(command)
    for root, _, files in os.walk(temp_directory):
        for file in files:
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                yield file_path



def get_doonsec_url():
    '''ä» Doonsec RSS è·å–ä»Šæ—¥URLã€æ—¥æœŸå’Œæ ‡é¢˜ï¼Œè¿”å›(url, date, title)å…ƒç»„åˆ—è¡¨'''
    logger.info("å¼€å§‹è·å–Doonsec RSS")
    cookies = {
        'UM_follow': 'True',
        'UM_distinctids': 'fgmr',
        'session': 'eyJfcGVybWFuZW50Ijp0cnVlLCJjc3JmX3Rva2VuIjoiMzU2ZDE4OTcwZjliZDljY2NjN2M3YzlkMzRhOGVlZWQyZDk1NmI1ZSIsInZpc3RvciI6ImZHTXJGQXBlVndRUnZrWjJHdWplV2gifQ.ZzidRw.GyjS15N12JYU0TByO31rrwBIiPY',
    }
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
        'cache-control': 'no-cache',
        'pragma': 'no-cache',
        'priority': 'u=0, i',
        'sec-ch-ua': '"Chromium";v="130", "Microsoft Edge";v="130", "Not?A_Brand";v="99"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0',
    }
    try:
        response = requests.get('https://wechat.doonsec.com/rss.xml', cookies=cookies, headers=headers)
        response.encoding = response.apparent_encoding
        logger.info("Doonsec RSSè¯·æ±‚æˆåŠŸ")
        root = ET.fromstring(response.text)
        url_date_title_list = []
        total_items = len(root.findall('./channel/item'))
        logger.info(f"RSSä¸­å…±æœ‰ {total_items} ä¸ªæ¡ç›®")
        
        for item in root.findall('./channel/item'):
            title = item.findtext('title') or ''
            link = item.findtext('link') or ''
            pub_date = item.findtext('pubDate') or ''
            date_str = ''
            if pub_date:
                try:
                    date_str = pub_date[:10]
                    logger.debug(f"è§£ææ—¥æœŸ: {pub_date} -> {date_str}")
                except:
                    date_str = ''
            
            # åªæ£€æŸ¥æ˜¯å¦ä¸ºå¾®ä¿¡é“¾æ¥ï¼Œä¸è¿›è¡Œå…³é”®è¯è¿‡æ»¤
            if link.startswith('https://mp.weixin.qq.com/'):
                url_date_title_list.append((link.rstrip(')'), date_str, title))
                logger.debug(f"è·å–åˆ°æ–‡ç« : {title} -> {link}")
        
        logger.info(f"Doonsecè·å–åˆ° {len(url_date_title_list)} ä¸ªå¾®ä¿¡æ–‡ç« URL")
        return url_date_title_list
    except Exception as e:
        logger.error(f"Doonsec RSSè§£æå¤±è´¥: {e}")
        return []

def parse_md_urls_with_title(md_text):
    """
    è§£æmdæ–‡ä»¶ï¼Œè¿”å›[(url, title)]
    æ”¯æŒ- [æ ‡é¢˜](url)ã€* [æ ‡é¢˜](url)ã€1. [æ ‡é¢˜](url)ç­‰æ ¼å¼
    """
    pattern = re.compile(r'^[\-\*\d\. ]+\[(.*?)\]\((https://mp.weixin.qq.com/[^)\s]+)\)', re.MULTILINE)
    return [(m.group(2), m.group(1)) for m in pattern.finditer(md_text)]

def filter_by_keywords(urls_info):
    """
    æ ¹æ®å…³é”®è¯è¿‡æ»¤æ–‡ç« ï¼Œåªä¿ç•™å®‰å…¨ç›¸å…³çš„æ–‡ç« 
    """
    # é€šè¿‡AIåˆ†æï¼Œå°†æ‰€æœ‰å…³é”®è¯æŒ‰åŠŸèƒ½é¢†åŸŸåˆ†ç±»
    keywords = [
        # ===== æ¼æ´åˆ©ç”¨ä¸æ”»å‡»æŠ€æœ¯ =====
        'å¤ç°', 'æ¼æ´', 'æ¼æ´åˆ©ç”¨', 'æ¼æ´æŒ–æ˜', 'æ¼æ´æ£€æµ‹', 'æ¼æ´åˆ†æ', 'æ¼æ´ä¿®å¤', 'æ¼æ´é˜²æŠ¤',
        'æ¼æ´æ‰«æ', 'æ¼æ´è¯„ä¼°', 'æ¼æ´ç®¡ç†', 'æ¼æ´å“åº”', 'æ¼æ´é¢„è­¦', 'æ¼æ´é€šæŠ¥',
        'SQLæ³¨å…¥', 'XSSæ”»å‡»', 'CSRFæ”»å‡»', 'æ–‡ä»¶ä¸Šä¼ ', 'æ–‡ä»¶åŒ…å«', 'å‘½ä»¤æ³¨å…¥',
        'ä»£ç æ³¨å…¥', 'ååºåˆ—åŒ–', 'ç¼“å†²åŒºæº¢å‡º', 'æƒé™æå‡', 'è¶Šæƒè®¿é—®', 'æœªæˆæƒè®¿é—®',
        'é€»è¾‘æ¼æ´', 'é…ç½®é”™è¯¯', 'å¼±å£ä»¤', 'é»˜è®¤å¯†ç ', 'ç¡¬ç¼–ç ', 'æ•æ„Ÿä¿¡æ¯æ³„éœ²',
        'æ³¨å…¥', 'XSS', 'å†…ç½‘', 'åŸŸæ§', 'RCE', 'ä»£ç æ‰§è¡Œ', 'å‘½ä»¤æ‰§è¡Œ',
        'è¿œç¨‹ä»£ç æ‰§è¡Œ', 'æœ¬åœ°ä»£ç æ‰§è¡Œ', 'æƒé™ç»•è¿‡', 'ä¿¡æ¯æ³„éœ²', 'æ‹’ç»æœåŠ¡',
        'å†…å­˜ç ´å', 'æ•´æ•°æº¢å‡º', 'æ ¼å¼åŒ–å­—ç¬¦ä¸²', 'ç«äº‰æ¡ä»¶', 'æ—¶é—´ç«äº‰',
        'è·¯å¾„éå†', 'ç›®å½•éå†', 'æ–‡ä»¶åŒ…å«', 'å‘½ä»¤æ³¨å…¥', 'ä»£ç æ³¨å…¥',
        
        # ===== å¨èƒæƒ…æŠ¥ä¸APT =====
        'å¨èƒæƒ…æŠ¥', 'å¨èƒæ£€æµ‹', 'å¨èƒç‹©çŒ', 'å¨èƒåˆ†æ', 'å¨èƒå»ºæ¨¡', 'å¨èƒè¯„ä¼°', 'å¨èƒé¢„è­¦',
        'æƒ…æŠ¥æ”¶é›†', 'æƒ…æŠ¥åˆ†æ', 'æƒ…æŠ¥å…±äº«', 'æƒ…æŠ¥å¹³å°', 'æƒ…æŠ¥ç³»ç»Ÿ', 'æƒ…æŠ¥è¿è¥',
        'æ¶æ„è½¯ä»¶', 'æ¶æ„ä»£ç ', 'æ¶æ„è¡Œä¸º', 'æ¶æ„æ´»åŠ¨', 'æ¶æ„æ”»å‡»', 'æ¶æ„å¨èƒ',
        'APTæ”»å‡»', 'APTç»„ç»‡', 'APTæ´»åŠ¨', 'APTå¨èƒ', 'APTæ£€æµ‹', 'APTåˆ†æ',
        'å¨èƒæƒ…æŠ¥å¹³å°', 'å¨èƒæƒ…æŠ¥ç³»ç»Ÿ', 'å¨èƒæƒ…æŠ¥åˆ†æ', 'å¨èƒæƒ…æŠ¥å…±äº«',
        
        # ===== åº”æ€¥å“åº”ä¸æº¯æº =====
        'åº”æ€¥å“åº”', 'å®‰å…¨å“åº”', 'äº‹ä»¶å“åº”', 'åº”æ€¥å¤„ç†', 'åº”æ€¥ç®¡ç†', 'åº”æ€¥æ¼”ç»ƒ',
        'æº¯æºåˆ†æ', 'æ”»å‡»æº¯æº', 'å¨èƒæº¯æº', 'æ¶æ„ä»£ç æº¯æº', 'ç½‘ç»œæº¯æº', 'æ•°å­—å–è¯',
        'å–è¯åˆ†æ', 'è¯æ®æ”¶é›†', 'è¯æ®ä¿å…¨', 'è¯æ®é“¾', 'æ—¶é—´çº¿åˆ†æ', 'æ”»å‡»é“¾åˆ†æ',
        'å¨èƒç‹©çŒ', 'å¨èƒè¿½è¸ª', 'å¨èƒå®šä½', 'å¨èƒè¯†åˆ«', 'å¨èƒåˆ†ç±»', 'å¨èƒè¯„ä¼°',
        'å®‰å…¨äº‹ä»¶', 'å®‰å…¨å‘Šè­¦', 'å®‰å…¨æ—¥å¿—', 'å®‰å…¨ç›‘æ§', 'å®‰å…¨æ£€æµ‹', 'å®‰å…¨åˆ†æ',
        
        # ===== å®‰å…¨è¿è¥ä¸ç®¡ç† =====
        'å®‰å…¨è¿è¥', 'å®‰å…¨è¿ç»´', 'å®‰å…¨ç®¡ç†', 'å®‰å…¨æ²»ç†', 'å®‰å…¨åˆè§„', 'å®‰å…¨å®¡è®¡',
        'å®‰å…¨ç›‘æ§', 'å®‰å…¨åˆ†æ', 'å®‰å…¨è¯„ä¼°', 'å®‰å…¨æµ‹è¯•', 'å®‰å…¨åŸ¹è®­', 'å®‰å…¨æ„è¯†',
        'å®‰å…¨æ¶æ„', 'å®‰å…¨è®¾è®¡', 'å®‰å…¨å¼€å‘', 'å®‰å…¨éƒ¨ç½²', 'å®‰å…¨é…ç½®', 'å®‰å…¨ç­–ç•¥',
        'å®‰å…¨æ§åˆ¶', 'å®‰å…¨é˜²æŠ¤', 'å®‰å…¨æ£€æµ‹', 'å®‰å…¨å“åº”', 'å®‰å…¨æ¢å¤', 'å®‰å…¨å¤‡ä»½',
        'å®‰å…¨æ—¥å¿—', 'å®‰å…¨äº‹ä»¶', 'å®‰å…¨å‘Šè­¦', 'å®‰å…¨æŠ¥å‘Š', 'å®‰å…¨æŒ‡æ ‡', 'å®‰å…¨åº¦é‡',
        'å®‰å…¨å·¥å…·', 'å®‰å…¨å¹³å°', 'å®‰å…¨ç³»ç»Ÿ', 'å®‰å…¨æœåŠ¡', 'å®‰å…¨å’¨è¯¢', 'å®‰å…¨å¤–åŒ…',
        'å®‰å…¨å›¢é˜Ÿ', 'å®‰å…¨ä¸“å®¶', 'å®‰å…¨å·¥ç¨‹å¸ˆ', 'å®‰å…¨åˆ†æå¸ˆ', 'å®‰å…¨ç®¡ç†å‘˜',
        'æ¼æ´è¿è¥', 'SRC', 'å®‰å…¨è¿è¥æ¡†æ¶', 'å®‰å…¨æ²»ç†æ¡†æ¶',
        
        # ===== çº¢é˜Ÿè“é˜Ÿä¸æ”»é˜²æ¼”ç»ƒ =====
        'çº¢é˜Ÿ', 'è“é˜Ÿ', 'ç´«é˜Ÿ', 'æ”»é˜²æ¼”ç»ƒ', 'æ¸—é€æµ‹è¯•', 'å®‰å…¨è¯„ä¼°',
        'æ¼æ´æ‰«æ', 'å®‰å…¨æµ‹è¯•', 'å®‰å…¨å®¡è®¡', 'å®‰å…¨è¯„ä¼°', 'é£é™©è¯„ä¼°',
        
        # ===== ç‰¹å®šæ”»å‡»æŠ€æœ¯ä¸æ¶æ„è½¯ä»¶ =====
        'ç¤¾ä¼šå·¥ç¨‹å­¦', 'é’“é±¼æ”»å‡»', 'æ°´å‘æ”»å‡»', 'ä¾›åº”é“¾æ”»å‡»', 'é›¶æ—¥æ”»å‡»',
        'ä¾§ä¿¡é“æ”»å‡»', 'ä¸­é—´äººæ”»å‡»', 'æ‹’ç»æœåŠ¡', 'åˆ†å¸ƒå¼æ‹’ç»æœåŠ¡', 'DDoS',
        'å‹’ç´¢è½¯ä»¶', 'æœ¨é©¬', 'åé—¨', 'ç—…æ¯’', 'è •è™«', 'åƒµå°¸ç½‘ç»œ', 'é“¶ç‹',
        
        # ===== æ¼æ´ç¼–å·ä¸æ ‡å‡† =====
        'CVE-', 'CNVD-', 'CNNVD-', 'XVE-', 'QVD-', 'POC', 'EXP', '0day', '1day', 'nday',
        'CWE-', 'ISO27001', 'NIST', 'OWASP', 'CIS', 'SOC', 'SIEM', 'SOAR',
        'å¨èƒæƒ…æŠ¥æ ‡å‡†', 'å®‰å…¨è¿è¥æ¡†æ¶', 'å®‰å…¨æ²»ç†æ¡†æ¶',
        
        # ===== æ•°æ®å®‰å…¨ä¸éšç§ =====
        'ä¿¡æ¯æ³„æ¼', 'æ•°æ®æ³„éœ²', 'éšç§æ³„éœ²', 'æ•°æ®å®‰å…¨', 'éšç§ä¿æŠ¤',
        'èº«ä»½è®¤è¯', 'è®¿é—®æ§åˆ¶', 'ä¼šè¯ç®¡ç†', 'åŠ å¯†ç®—æ³•', 'åŠ å¯†åè®®', 'æ•°å­—ç­¾å',
        'è¯ä¹¦ç®¡ç†', 'å¯†é’¥ç®¡ç†', 'å¯†ç å­¦', 'å¯†ç ç ´è§£', 'å¤šå› å­è®¤è¯', 'å•ç‚¹ç™»å½•',
        
        # ===== äº‘å®‰å…¨ä¸æ–°å…´æŠ€æœ¯ =====
        'äº‘å®‰å…¨', 'å®¹å™¨å®‰å…¨', 'DevSecOps', 'äº‘åŸç”Ÿå®‰å…¨', 'å¾®æœåŠ¡å®‰å…¨',
        'åŒºå—é“¾å®‰å…¨', 'äººå·¥æ™ºèƒ½å®‰å…¨', 'æœºå™¨å­¦ä¹ å®‰å…¨', 'æ·±åº¦å­¦ä¹ å®‰å…¨',
        'é‡å­è®¡ç®—å¨èƒ', 'AIå®‰å…¨å¨èƒ', '5Gå®‰å…¨å¨èƒ', 'è¾¹ç¼˜è®¡ç®—å®‰å…¨',
        'é›¶ä¿¡ä»»æ¶æ„', 'å¾®åˆ†æ®µ', 'å¾®éš”ç¦»', 'è‡ªé€‚åº”å®‰å…¨', 'æ™ºèƒ½å®‰å…¨',
        
        # ===== åº”ç”¨ä¸ç³»ç»Ÿå®‰å…¨ =====
        'åº”ç”¨å®‰å…¨', 'Webå®‰å…¨', 'ç§»åŠ¨å®‰å…¨', 'Webåº”ç”¨å®‰å…¨', 'ç§»åŠ¨åº”ç”¨å®‰å…¨', 'APIå®‰å…¨',
        'Windowså®‰å…¨', 'Linuxå®‰å…¨', 'macOSå®‰å…¨', 'Androidå®‰å…¨', 'iOSå®‰å…¨',
        
        # ===== è¡Œä¸šä¸åŸºç¡€è®¾æ–½å®‰å…¨ =====
        'ç‰©è”ç½‘å®‰å…¨', 'å·¥ä¸šå®‰å…¨', 'ä¾›åº”é“¾å®‰å…¨', 'é‡‘èå®‰å…¨', 'åŒ»ç–—å®‰å…¨', 'æ•™è‚²å®‰å…¨',
        'æ”¿åºœå®‰å…¨', 'ä¼ä¸šå®‰å…¨', 'å…³é”®åŸºç¡€è®¾æ–½å®‰å…¨', 'å·¥ä¸šæ§åˆ¶ç³»ç»Ÿå®‰å…¨', 'æ™ºèƒ½ç”µç½‘å®‰å…¨',
        
        # ===== å®‰å…¨å·¥å…·ä¸æŠ€æœ¯ =====
        'é˜²ç«å¢™', 'å…¥ä¾µæ£€æµ‹', 'å…¥ä¾µé˜²æŠ¤', 'å®‰å…¨ç½‘å…³', 'VPN', 'åŠ å¯†',
        'å®¡è®¡æ—¥å¿—', 'å®‰å…¨æ‰«æ', 'æ¼æ´æ‰«æ', 'æ¸—é€æµ‹è¯•', 'ä»£ç å®¡è®¡', 'å®‰å…¨è¯„ä¼°'
    ]
    
    filtered_urls = []
    skipped_count = 0
    
    for url, source, title, date in urls_info:
        if not title:
            continue
            
        title_lower = title.lower()
        matched = False
        
        for keyword in keywords:
            if keyword.lower() in title_lower:
                filtered_urls.append((url, source, title, date))
                logger.debug(f"å…³é”®è¯åŒ¹é…: {keyword} -> {title}")
                matched = True
                break
        
        if not matched:
            logger.debug(f"å…³é”®è¯ä¸åŒ¹é…ï¼Œè·³è¿‡: {title}")
            skipped_count += 1
    
    logger.info(f"å…³é”®è¯è¿‡æ»¤: åŒ¹é… {len(filtered_urls)} ä¸ªï¼Œè·³è¿‡ {skipped_count} ä¸ª")
    return filtered_urls

def process_one_day(date_str, doonsec_list, chainreactors_urls, brucefeiix_urls, data, data_file, base_result_path, executable_path):
    # 1. å…ˆå»é‡ï¼Œæ”¶é›†æ‰€æœ‰å¾…å¤„ç†ä¿¡æ¯ï¼ˆå¸¦æ ‡é¢˜ï¼‰
    logger.info(f"=== å¼€å§‹å¤„ç† {date_str} çš„æ•°æ® ===")
    logger.info(f"DoonsecåŸå§‹æ•°æ®: {len(doonsec_list)} ä¸ª")
    logger.info(f"ChainReactorsåŸå§‹æ•°æ®: {len(chainreactors_urls)} ä¸ª")
    logger.info(f"BruceFeIixåŸå§‹æ•°æ®: {len(brucefeiix_urls)} ä¸ª")
    
    urls_info = []
    url_set = set()
    skipped_count = 0
    
    # Doonsec
    logger.info("å¼€å§‹å¤„ç†Doonsecæ•°æ®...")
    for url, ddate, title in doonsec_list:
        use_date = ddate if ddate else date_str
        if url in data or url in url_set:
            logger.debug(f"è·³è¿‡å·²å­˜åœ¨çš„URL: {url}")
            skipped_count += 1
            continue
        urls_info.append((url, "Doonsec", title, use_date))
        url_set.add(url)
        logger.debug(f"æ·»åŠ Doonsec URL: {url}")
    
    # ChainReactors
    logger.info("å¼€å§‹å¤„ç†ChainReactorsæ•°æ®...")
    for url, title in chainreactors_urls:
        if url in data or url in url_set:
            logger.debug(f"è·³è¿‡å·²å­˜åœ¨çš„URL: {url}")
            skipped_count += 1
            continue
        urls_info.append((url, "ChainReactors", title, date_str))
        url_set.add(url)
        logger.debug(f"æ·»åŠ ChainReactors URL: {url}")
    
    # BruceFeIix
    logger.info("å¼€å§‹å¤„ç†BruceFeIixæ•°æ®...")
    for url, title in brucefeiix_urls:
        if url in data or url in url_set:
            logger.debug(f"è·³è¿‡å·²å­˜åœ¨çš„URL: {url}")
            skipped_count += 1
            continue
        urls_info.append((url, "BruceFeIix", title, date_str))
        url_set.add(url)
        logger.debug(f"æ·»åŠ BruceFeIix URL: {url}")
    
    logger.info(f"å»é‡åå…± {len(urls_info)} ä¸ªURLå¾…å¤„ç†ï¼Œè·³è¿‡ {skipped_count} ä¸ªé‡å¤URL")
    
    # æŒ‰æ•°æ®æºç»Ÿè®¡
    doonsec_count = len([u for u in urls_info if u[1] == "Doonsec"])
    chainreactors_count = len([u for u in urls_info if u[1] == "ChainReactors"])
    brucefeiix_count = len([u for u in urls_info if u[1] == "BruceFeIix"])
    logger.info(f"å»é‡åç»Ÿè®¡ - Doonsec: {doonsec_count} ä¸ª, ChainReactors: {chainreactors_count} ä¸ª, BruceFeIix: {brucefeiix_count} ä¸ª")
    
    # 2. å…³é”®è¯è¿‡æ»¤
    logger.info("=== å¼€å§‹å…³é”®è¯è¿‡æ»¤ ===")
    urls_info = filter_by_keywords(urls_info)
    
    # è¿‡æ»¤åæŒ‰æ•°æ®æºç»Ÿè®¡
    doonsec_count = len([u for u in urls_info if u[1] == "Doonsec"])
    chainreactors_count = len([u for u in urls_info if u[1] == "ChainReactors"])
    brucefeiix_count = len([u for u in urls_info if u[1] == "BruceFeIix"])
    logger.info(f"å…³é”®è¯è¿‡æ»¤åç»Ÿè®¡ - Doonsec: {doonsec_count} ä¸ª, ChainReactors: {chainreactors_count} ä¸ª, BruceFeIix: {brucefeiix_count} ä¸ª")
    
    # 3. å…ˆç”Ÿæˆå½“æ—¥mdæŠ¥å‘Šï¼ˆæ ‡é¢˜å’Œé“¾æ¥åŒæ­¥ï¼‰
    if urls_info:
        create_daily_md_report(date_str, urls_info)
    # 4. å†æ‰¹é‡æŠ“å–å’Œå½’æ¡£
    for idx, (url, source, title, article_date) in enumerate(urls_info):
        real_title = save_md_and_update_data(url, article_date, base_result_path, data, data_file, executable_path, source, article_date)
        if not title:
            urls_info[idx] = (url, source, real_title, article_date)
    # 5. æœ€åå†è¡¥å…¨mdæŠ¥å‘Šï¼ˆå¸¦çœŸå®æ ‡é¢˜ï¼‰
    if urls_info:
        create_daily_md_report(date_str, urls_info)



    
def rep_filename(result_path):
    ''' 
    æ›¿æ¢ä¸èƒ½ç”¨äºæ–‡ä»¶åçš„å­—ç¬¦
    '''
    for root, _, files in os.walk(result_path):
        for file in files:
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                new_file = re.sub(r'[\/\\\:\*\?\"\<\>\|]', '', file)
                shutil.move(os.path.join(root, file), os.path.join(root, new_file))
                

def extract_title_from_md(md_path):
    # å°è¯•ä»mdæ–‡ä»¶é¦–è¡Œè·å–æ ‡é¢˜ï¼Œå¦åˆ™ç”¨æ–‡ä»¶å
    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            first_line = f.readline().strip()
            if first_line.startswith('#'):
                return first_line.lstrip('#').strip()
    except:
        pass
    return os.path.splitext(os.path.basename(md_path))[0]

def analyze_security_threats(urls_info):
    """
    åˆ†æå®‰å…¨å¨èƒæ€åŠ¿
    """
    threat_categories = {
        'æ¼æ´åˆ©ç”¨': ['CVE', 'CNVD', 'CNNVD', 'XVE', 'QVD', 'POC', 'EXP', '0day', '1day', 'nday', 'æ¼æ´', 'å¤ç°'],
        'æ”»å‡»æŠ€æœ¯': ['æ³¨å…¥', 'XSS', 'RCE', 'ä»£ç æ‰§è¡Œ', 'å‘½ä»¤æ‰§è¡Œ', 'å†…ç½‘', 'åŸŸæ§'],
        'å¨èƒæƒ…æŠ¥': ['å¨èƒæƒ…æŠ¥', 'APT', 'é“¶ç‹', 'å‹’ç´¢ç—…æ¯’', 'åº”æ€¥å“åº”'],
        'å®‰å…¨è¿è¥': ['å®‰å…¨è¿è¥', 'æ¼æ´è¿è¥', 'æƒ…æŠ¥è¿è¥', 'SRC'],
        'ä¿¡æ¯æ³„éœ²': ['ä¿¡æ¯æ³„æ¼', 'æ•°æ®æ³„éœ²', 'é…ç½®æ³„éœ²'],
        'ä¾›åº”é“¾': ['ä¾›åº”é“¾', 'ç¬¬ä¸‰æ–¹', 'ç»„ä»¶']
    }
    
    threat_stats = {category: 0 for category in threat_categories.keys()}
    threat_details = {category: [] for category in threat_categories.keys()}
    
    for url, source, title, date in urls_info:
        if not title:
            continue
        title_lower = title.lower()
        for category, keywords in threat_categories.items():
            for keyword in keywords:
                if keyword.lower() in title_lower:
                    threat_stats[category] += 1
                    threat_details[category].append((title, source, url))
                    break
    
    return threat_stats, threat_details

def analyze_vulnerability_types(urls_info):
    """
    åˆ†ææ¼æ´ç±»å‹åˆ†å¸ƒ
    """
    vuln_types = {
        'Webå®‰å…¨': ['SQLæ³¨å…¥', 'XSS', 'CSRF', 'æ–‡ä»¶ä¸Šä¼ ', 'æ–‡ä»¶åŒ…å«', 'å‘½ä»¤æ³¨å…¥'],
        'ç³»ç»Ÿæ¼æ´': ['RCE', 'æƒé™æå‡', 'ç¼“å†²åŒºæº¢å‡º', 'å†…æ ¸æ¼æ´'],
        'åº”ç”¨æ¼æ´': ['ååºåˆ—åŒ–', 'é€»è¾‘æ¼æ´', 'é…ç½®é”™è¯¯', 'å¼±å£ä»¤'],
        'ç½‘ç»œæ”»å‡»': ['é’“é±¼', 'ç¤¾ä¼šå·¥ç¨‹å­¦', 'APT', 'å‹’ç´¢è½¯ä»¶'],
        'ä¾›åº”é“¾': ['ç¬¬ä¸‰æ–¹ç»„ä»¶', 'å¼€æºæ¼æ´', 'ä¾èµ–æ³¨å…¥']
    }
    
    vuln_stats = {vuln_type: 0 for vuln_type in vuln_types.keys()}
    
    for url, source, title, date in urls_info:
        if not title:
            continue
        title_lower = title.lower()
        for vuln_type, keywords in vuln_types.items():
            for keyword in keywords:
                if keyword.lower() in title_lower:
                    vuln_stats[vuln_type] += 1
                    break
    
    return vuln_stats

def create_daily_md_report(date_str, urls_info, md_dir="md"):
    """
    åˆ›å»ºæ¯æ—¥mdæŠ¥å‘Šæ–‡æ¡£
    urls_info: [(url, source, title, date), ...]
    """
    os.makedirs(md_dir, exist_ok=True)
    
    # æ–‡ä»¶åæ ¼å¼ï¼š2025-07-25.md
    filename = f"{date_str}.md"
    filepath = os.path.join(md_dir, filename)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_urls = len(urls_info)
    sources = {}
    for _, source, _, _ in urls_info:
        sources[source] = sources.get(source, 0) + 1
    
    # å®‰å…¨å¨èƒåˆ†æ
    threat_stats, threat_details = analyze_security_threats(urls_info)
    vuln_stats = analyze_vulnerability_types(urls_info)
    
    # MDæ¨¡æ¿
    md_content = f"""# {date_str} å®‰å…¨å¨èƒæ€åŠ¿æŠ¥å‘Š

## ğŸ“Š æ•°æ®æ¦‚è§ˆ

- **æ€»æ–‡ç« æ•°**: {total_urls}
- **æ•°æ®æºåˆ†å¸ƒ**:
"""
    
    for source, count in sources.items():
        md_content += f"  - {source}: {count}ç¯‡\n"
    
    md_content += f"""
## ğŸš¨ å®‰å…¨å¨èƒæ€åŠ¿åˆ†æ

### å¨èƒç±»å‹åˆ†å¸ƒ
"""
    
    # æŒ‰å¨èƒæ•°é‡æ’åº
    sorted_threats = sorted(threat_stats.items(), key=lambda x: x[1], reverse=True)
    for threat_type, count in sorted_threats:
        if count > 0:
            md_content += f"- **{threat_type}**: {count}ç¯‡\n"
    
    md_content += f"""
### æ¼æ´ç±»å‹åˆ†æ
"""
    
    # æŒ‰æ¼æ´æ•°é‡æ’åº
    sorted_vulns = sorted(vuln_stats.items(), key=lambda x: x[1], reverse=True)
    for vuln_type, count in sorted_vulns:
        if count > 0:
            md_content += f"- **{vuln_type}**: {count}ç¯‡\n"
    
    md_content += f"""
## ğŸ” åŒ¹é…è§„åˆ™

### å…³é”®è¯åŒ¹é…è§„åˆ™

#### ğŸ” æ¼æ´åˆ©ç”¨ä¸æ”»å‡»æŠ€æœ¯
`å¤ç°|æ¼æ´|æ¼æ´åˆ©ç”¨|æ¼æ´æŒ–æ˜|æ¼æ´æ£€æµ‹|æ¼æ´åˆ†æ|æ¼æ´ä¿®å¤|æ¼æ´é˜²æŠ¤|æ¼æ´æ‰«æ|æ¼æ´è¯„ä¼°|æ¼æ´ç®¡ç†|æ¼æ´å“åº”|æ¼æ´é¢„è­¦|æ¼æ´é€šæŠ¥|SQLæ³¨å…¥|XSSæ”»å‡»|CSRFæ”»å‡»|æ–‡ä»¶ä¸Šä¼ |æ–‡ä»¶åŒ…å«|å‘½ä»¤æ³¨å…¥|ä»£ç æ³¨å…¥|ååºåˆ—åŒ–|ç¼“å†²åŒºæº¢å‡º|æƒé™æå‡|è¶Šæƒè®¿é—®|æœªæˆæƒè®¿é—®|é€»è¾‘æ¼æ´|é…ç½®é”™è¯¯|å¼±å£ä»¤|é»˜è®¤å¯†ç |ç¡¬ç¼–ç |æ•æ„Ÿä¿¡æ¯æ³„éœ²|æ³¨å…¥|XSS|å†…ç½‘|åŸŸæ§|RCE|ä»£ç æ‰§è¡Œ|å‘½ä»¤æ‰§è¡Œ|è¿œç¨‹ä»£ç æ‰§è¡Œ|æœ¬åœ°ä»£ç æ‰§è¡Œ|æƒé™ç»•è¿‡|ä¿¡æ¯æ³„éœ²|æ‹’ç»æœåŠ¡|å†…å­˜ç ´å|æ•´æ•°æº¢å‡º|æ ¼å¼åŒ–å­—ç¬¦ä¸²|ç«äº‰æ¡ä»¶|æ—¶é—´ç«äº‰|è·¯å¾„éå†|ç›®å½•éå†|æ–‡ä»¶åŒ…å«|å‘½ä»¤æ³¨å…¥|ä»£ç æ³¨å…¥`

#### ğŸ•µï¸ å¨èƒæƒ…æŠ¥ä¸APT
`å¨èƒæƒ…æŠ¥|å¨èƒæ£€æµ‹|å¨èƒç‹©çŒ|å¨èƒåˆ†æ|å¨èƒå»ºæ¨¡|å¨èƒè¯„ä¼°|å¨èƒé¢„è­¦|æƒ…æŠ¥æ”¶é›†|æƒ…æŠ¥åˆ†æ|æƒ…æŠ¥å…±äº«|æƒ…æŠ¥å¹³å°|æƒ…æŠ¥ç³»ç»Ÿ|æƒ…æŠ¥è¿è¥|æ¶æ„è½¯ä»¶|æ¶æ„ä»£ç |æ¶æ„è¡Œä¸º|æ¶æ„æ´»åŠ¨|æ¶æ„æ”»å‡»|æ¶æ„å¨èƒ|APTæ”»å‡»|APTç»„ç»‡|APTæ´»åŠ¨|APTå¨èƒ|APTæ£€æµ‹|APTåˆ†æ|å¨èƒæƒ…æŠ¥å¹³å°|å¨èƒæƒ…æŠ¥ç³»ç»Ÿ|å¨èƒæƒ…æŠ¥åˆ†æ|å¨èƒæƒ…æŠ¥å…±äº«`

#### ğŸš¨ åº”æ€¥å“åº”ä¸æº¯æº
`åº”æ€¥å“åº”|å®‰å…¨å“åº”|äº‹ä»¶å“åº”|åº”æ€¥å¤„ç†|åº”æ€¥ç®¡ç†|åº”æ€¥æ¼”ç»ƒ|æº¯æºåˆ†æ|æ”»å‡»æº¯æº|å¨èƒæº¯æº|æ¶æ„ä»£ç æº¯æº|ç½‘ç»œæº¯æº|æ•°å­—å–è¯|å–è¯åˆ†æ|è¯æ®æ”¶é›†|è¯æ®ä¿å…¨|è¯æ®é“¾|æ—¶é—´çº¿åˆ†æ|æ”»å‡»é“¾åˆ†æ|å¨èƒç‹©çŒ|å¨èƒè¿½è¸ª|å¨èƒå®šä½|å¨èƒè¯†åˆ«|å¨èƒåˆ†ç±»|å¨èƒè¯„ä¼°|å®‰å…¨äº‹ä»¶|å®‰å…¨å‘Šè­¦|å®‰å…¨æ—¥å¿—|å®‰å…¨ç›‘æ§|å®‰å…¨æ£€æµ‹|å®‰å…¨åˆ†æ`

#### ğŸ›¡ï¸ å®‰å…¨è¿è¥ä¸ç®¡ç†
`å®‰å…¨è¿è¥|å®‰å…¨è¿ç»´|å®‰å…¨ç®¡ç†|å®‰å…¨æ²»ç†|å®‰å…¨åˆè§„|å®‰å…¨å®¡è®¡|å®‰å…¨ç›‘æ§|å®‰å…¨åˆ†æ|å®‰å…¨è¯„ä¼°|å®‰å…¨æµ‹è¯•|å®‰å…¨åŸ¹è®­|å®‰å…¨æ„è¯†|å®‰å…¨æ¶æ„|å®‰å…¨è®¾è®¡|å®‰å…¨å¼€å‘|å®‰å…¨éƒ¨ç½²|å®‰å…¨é…ç½®|å®‰å…¨ç­–ç•¥|å®‰å…¨æ§åˆ¶|å®‰å…¨é˜²æŠ¤|å®‰å…¨æ£€æµ‹|å®‰å…¨å“åº”|å®‰å…¨æ¢å¤|å®‰å…¨å¤‡ä»½|å®‰å…¨æ—¥å¿—|å®‰å…¨äº‹ä»¶|å®‰å…¨å‘Šè­¦|å®‰å…¨æŠ¥å‘Š|å®‰å…¨æŒ‡æ ‡|å®‰å…¨åº¦é‡|å®‰å…¨å·¥å…·|å®‰å…¨å¹³å°|å®‰å…¨ç³»ç»Ÿ|å®‰å…¨æœåŠ¡|å®‰å…¨å’¨è¯¢|å®‰å…¨å¤–åŒ…|å®‰å…¨å›¢é˜Ÿ|å®‰å…¨ä¸“å®¶|å®‰å…¨å·¥ç¨‹å¸ˆ|å®‰å…¨åˆ†æå¸ˆ|å®‰å…¨ç®¡ç†å‘˜|æ¼æ´è¿è¥|SRC|å®‰å…¨è¿è¥æ¡†æ¶|å®‰å…¨æ²»ç†æ¡†æ¶`

#### âš”ï¸ çº¢é˜Ÿè“é˜Ÿä¸æ”»é˜²æ¼”ç»ƒ
`çº¢é˜Ÿ|è“é˜Ÿ|ç´«é˜Ÿ|æ”»é˜²æ¼”ç»ƒ|æ¸—é€æµ‹è¯•|å®‰å…¨è¯„ä¼°|æ¼æ´æ‰«æ|å®‰å…¨æµ‹è¯•|å®‰å…¨å®¡è®¡|å®‰å…¨è¯„ä¼°|é£é™©è¯„ä¼°`

#### ğŸ¦  ç‰¹å®šæ”»å‡»æŠ€æœ¯ä¸æ¶æ„è½¯ä»¶
`ç¤¾ä¼šå·¥ç¨‹å­¦|é’“é±¼æ”»å‡»|æ°´å‘æ”»å‡»|ä¾›åº”é“¾æ”»å‡»|é›¶æ—¥æ”»å‡»|ä¾§ä¿¡é“æ”»å‡»|ä¸­é—´äººæ”»å‡»|æ‹’ç»æœåŠ¡|åˆ†å¸ƒå¼æ‹’ç»æœåŠ¡|DDoS|å‹’ç´¢è½¯ä»¶|æœ¨é©¬|åé—¨|ç—…æ¯’|è •è™«|åƒµå°¸ç½‘ç»œ|é“¶ç‹`

#### ğŸ“‹ æ¼æ´ç¼–å·ä¸æ ‡å‡†
`CVE-|CNVD-|CNNVD-|XVE-|QVD-|POC|EXP|0day|1day|nday|CWE-|ISO27001|NIST|OWASP|CIS|SOC|SIEM|SOAR|å¨èƒæƒ…æŠ¥æ ‡å‡†|å®‰å…¨è¿è¥æ¡†æ¶|å®‰å…¨æ²»ç†æ¡†æ¶`

#### ğŸ” æ•°æ®å®‰å…¨ä¸éšç§
`ä¿¡æ¯æ³„æ¼|æ•°æ®æ³„éœ²|éšç§æ³„éœ²|æ•°æ®å®‰å…¨|éšç§ä¿æŠ¤|èº«ä»½è®¤è¯|è®¿é—®æ§åˆ¶|ä¼šè¯ç®¡ç†|åŠ å¯†ç®—æ³•|åŠ å¯†åè®®|æ•°å­—ç­¾å|è¯ä¹¦ç®¡ç†|å¯†é’¥ç®¡ç†|å¯†ç å­¦|å¯†ç ç ´è§£|å¤šå› å­è®¤è¯|å•ç‚¹ç™»å½•`

#### â˜ï¸ äº‘å®‰å…¨ä¸æ–°å…´æŠ€æœ¯
`äº‘å®‰å…¨|å®¹å™¨å®‰å…¨|DevSecOps|äº‘åŸç”Ÿå®‰å…¨|å¾®æœåŠ¡å®‰å…¨|åŒºå—é“¾å®‰å…¨|äººå·¥æ™ºèƒ½å®‰å…¨|æœºå™¨å­¦ä¹ å®‰å…¨|æ·±åº¦å­¦ä¹ å®‰å…¨|é‡å­è®¡ç®—å¨èƒ|AIå®‰å…¨å¨èƒ|5Gå®‰å…¨å¨èƒ|è¾¹ç¼˜è®¡ç®—å®‰å…¨|é›¶ä¿¡ä»»æ¶æ„|å¾®åˆ†æ®µ|å¾®éš”ç¦»|è‡ªé€‚åº”å®‰å…¨|æ™ºèƒ½å®‰å…¨`

#### ğŸ’» åº”ç”¨ä¸ç³»ç»Ÿå®‰å…¨
`åº”ç”¨å®‰å…¨|Webå®‰å…¨|ç§»åŠ¨å®‰å…¨|Webåº”ç”¨å®‰å…¨|ç§»åŠ¨åº”ç”¨å®‰å…¨|APIå®‰å…¨|Windowså®‰å…¨|Linuxå®‰å…¨|macOSå®‰å…¨|Androidå®‰å…¨|iOSå®‰å…¨`

#### ğŸ­ è¡Œä¸šä¸åŸºç¡€è®¾æ–½å®‰å…¨
`ç‰©è”ç½‘å®‰å…¨|å·¥ä¸šå®‰å…¨|ä¾›åº”é“¾å®‰å…¨|é‡‘èå®‰å…¨|åŒ»ç–—å®‰å…¨|æ•™è‚²å®‰å…¨|æ”¿åºœå®‰å…¨|ä¼ä¸šå®‰å…¨|å…³é”®åŸºç¡€è®¾æ–½å®‰å…¨|å·¥ä¸šæ§åˆ¶ç³»ç»Ÿå®‰å…¨|æ™ºèƒ½ç”µç½‘å®‰å…¨`

#### ğŸ› ï¸ å®‰å…¨å·¥å…·ä¸æŠ€æœ¯
`é˜²ç«å¢™|å…¥ä¾µæ£€æµ‹|å…¥ä¾µé˜²æŠ¤|å®‰å…¨ç½‘å…³|VPN|åŠ å¯†|å®¡è®¡æ—¥å¿—|å®‰å…¨æ‰«æ|æ¼æ´æ‰«æ|æ¸—é€æµ‹è¯•|ä»£ç å®¡è®¡|å®‰å…¨è¯„ä¼°`

### URLåŒ¹é…
`https://mp.weixin.qq.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*]|(?:%[0-9a-fA-F][0-9a-fA-F]))+`

## ğŸ“° æ–‡ç« è¯¦ç»†åˆ—è¡¨

"""
    
    # æŒ‰æ¥æºåˆ†ç»„
    source_groups = {}
    for url, source, title, article_date in urls_info:
        if source not in source_groups:
            source_groups[source] = []
        source_groups[source].append((url, title, article_date))
    
    for source, articles in source_groups.items():
        md_content += f"### {source}\n\n"
        for url, title, article_date in articles:
            date_info = f" (å‘å¸ƒæ—¥æœŸ: {article_date})" if article_date else ""
            md_content += f"- [{title}]({url}){date_info}\n"
        md_content += "\n"
    
    # å¨èƒè¯¦æƒ…
    md_content += f"""
## ğŸ¯ å¨èƒè¯¦æƒ…åˆ†æ

"""
    
    for threat_type, articles in threat_details.items():
        if articles:
            md_content += f"### {threat_type}\n\n"
            md_content += "| åºå· | æ–‡ç« æ ‡é¢˜ | æ¥æº | é“¾æ¥ |\n"
            md_content += "|------|----------|------|------|\n"
            for idx, (title, source, url) in enumerate(articles, 1):
                md_content += f"| {idx} | {title} | {source} | [{url}]({url}) |\n"
            md_content += "\n"
    
    md_content += f"""
## ğŸ“ å½’æ¡£è·¯å¾„

æ–‡ç« å·²å½’æ¡£åˆ°: `doc/{date_str[:4]}/{date_str[:7]}/{date_str[:4]}-W{datetime.datetime.strptime(date_str, '%Y-%m-%d').isocalendar()[1]:02d}/{date_str}/`

## ğŸ”— æ•°æ®æºè¯´æ˜

- **ChainReactors**: GitHubå®‰å…¨æ–‡ç« èšåˆï¼Œä¸“æ³¨äºæ¼æ´å¤ç°å’ŒæŠ€æœ¯åˆ†æ
- **BruceFeIix**: å®‰å…¨æ–‡ç« æ”¶é›†ï¼Œæ¶µç›–å¨èƒæƒ…æŠ¥å’Œå®‰å…¨è¿è¥
- **Doonsec**: å®‰å…¨èµ„è®¯RSSï¼Œå®æ—¶æ¨é€å®‰å…¨äº‹ä»¶å’Œæ¼æ´é¢„è­¦

## ğŸ“ˆ è¶‹åŠ¿åˆ†æ

### ä»Šæ—¥é‡ç‚¹å…³æ³¨
"""
    
    # æ‰¾å‡ºä»Šæ—¥æœ€çƒ­é—¨çš„å¨èƒç±»å‹
    if threat_stats:
        top_threat = max(threat_stats.items(), key=lambda x: x[1])
        md_content += f"- **{top_threat[0]}** æ˜¯ä»Šæ—¥ä¸»è¦å¨èƒç±»å‹ï¼Œå…± {top_threat[1]} ç¯‡ç›¸å…³æ–‡ç« \n"
    
    # æ‰¾å‡ºä»Šæ—¥æœ€çƒ­é—¨çš„æ¼æ´ç±»å‹
    if vuln_stats:
        top_vuln = max(vuln_stats.items(), key=lambda x: x[1])
        md_content += f"- **{top_vuln[0]}** æ˜¯ä»Šæ—¥ä¸»è¦æ¼æ´ç±»å‹ï¼Œå…± {top_vuln[1]} ç¯‡ç›¸å…³æ–‡ç« \n"
    
    md_content += f"""
### å®‰å…¨å»ºè®®
- åŠæ—¶å…³æ³¨é«˜å±æ¼æ´çš„ä¿®å¤è¿›å±•
- åŠ å¼ºä¾›åº”é“¾å®‰å…¨ç®¡ç†
- å®šæœŸè¿›è¡Œå®‰å…¨åŸ¹è®­å’Œæ„è¯†æå‡
- å»ºç«‹å®Œå–„çš„å®‰å…¨è¿è¥ä½“ç³»

---
*ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*æŠ¥å‘Šå·¥å…·: å¾®ä¿¡æ–‡ç« å®‰å…¨å½’æ¡£ç³»ç»Ÿ*
"""
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    logger.info(f"å·²åˆ›å»ºæ¯æ—¥æŠ¥å‘Š: {filepath}")
    return filepath

def save_md_and_update_data(url, date_str, base_result_path, data, data_file, executable_path, source="æœªçŸ¥", article_date=None):
    logger.info(f"å¼€å§‹å¤„ç†URL: {url}")
    logger.info(f"ç›®æ ‡æ—¥æœŸ: {date_str}")
    logger.info(f"æ•°æ®æº: {source}")
    
    dt = datetime.datetime.strptime(date_str, "%Y-%m-%d")
    year = dt.year
    month = dt.strftime('%Y-%m')
    week = f"{year}-W{dt.isocalendar()[1]:02d}"
    day = dt.strftime('%Y-%m-%d')
    result_path = os.path.join(base_result_path, str(year), month, week, day)
    
    logger.info(f"ç”Ÿæˆç›®å½•ç»“æ„: {result_path}")
    os.makedirs(result_path, exist_ok=True)
    
    title = "æœªçŸ¥æ ‡é¢˜"
    for file_path in get_md_path(executable_path, url):
        filename = os.path.basename(file_path)
        logger.info(f"å¤„ç†æ–‡ä»¶: {filename}")
        
        if filename == '.md':
            logger.warning(f"è·³è¿‡ç©ºæ–‡ä»¶: {filename}")
            continue
            
        shutil.copy2(file_path, result_path)
        logger.info(f"æ–‡ä»¶å·²å¤åˆ¶åˆ°: {result_path}")
        
        # è·å–æ ‡é¢˜
        title = extract_title_from_md(file_path)
        logger.info(f"æå–æ ‡é¢˜: {title}")
        
        # ä¿å­˜æ ‡é¢˜åˆ°data.json
        data[url] = title
        write_json(data_file, data)
        logger.info(f"å·²æ›´æ–°data.json: {url} -> {title}")
        
        print(title, end='ã€')
    
    rep_filename(result_path)
    logger.info(f"å®Œæˆå¤„ç†URL: {url}")
    
    return title

def get_chainreactors_md_url(date_str):
    """
    è·å–æŒ‡å®šæ—¥æœŸçš„ChainReactorsæ¯æ—¥mdæ–‡ä»¶URL
    """
    return f'https://raw.githubusercontent.com/chainreactors/picker/refs/heads/master/archive/daily/{date_str[:4]}/{date_str}.md'

def get_BruceFeIix_md_url(date_str):
    """
    è·å–æŒ‡å®šæ—¥æœŸçš„BruceFeIixæ¯æ—¥mdæ–‡ä»¶URL
    """
    return f'https://raw.githubusercontent.com/BruceFeIix/picker/refs/heads/master/archive/daily/{date_str[:4]}/{date_str}.md'

def main():
    '''ä¸»å‡½æ•°'''
    logger.info("=== å¼€å§‹æ‰§è¡Œå¾®ä¿¡æ–‡ç« å½’æ¡£å·¥å…· ===")
    
    parser = argparse.ArgumentParser(description='å¾®ä¿¡æ–‡ç« æ‰¹é‡å½’æ¡£å·¥å…·')
    parser.add_argument('--history', action='store_true', help='æ‹‰å–å†å²è®°å½•')
    parser.add_argument('--date', type=str, help='æŒ‡å®šæ—¥æœŸï¼Œæ ¼å¼YYYY-MM-DD')
    parser.add_argument('--range', nargs=2, metavar=('START', 'END'), help='æŒ‡å®šæ—¥æœŸåŒºé—´ï¼Œæ ¼å¼YYYY-MM-DD YYYY-MM-DD')
    args = parser.parse_args()

    data_file = 'data.json'
    data = {}
    executable_path = get_executable_path()
    base_result_path = 'doc'

    logger.info(f"æ•°æ®æ–‡ä»¶: {data_file}")
    logger.info(f"å¯æ‰§è¡Œæ–‡ä»¶: {executable_path}")
    logger.info(f"æ–‡æ¡£ç›®å½•: {base_result_path}")

    # è¯»å–å†å²è®°å½•
    data = read_json(data_file, default_data=data)
    logger.info(f"å·²åŠ è½½ {len(data)} æ¡å†å²è®°å½•")

    if args.history:
        logger.info("=== å¼€å§‹å†å²è®°å½•æ‹‰å– ===")
        start_date = '2022-04-07'
        end_date = datetime.datetime.now().strftime('%Y-%m-%d')
        start = datetime.datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.datetime.strptime(end_date, "%Y-%m-%d")
        current_date = start
        logger.info(f"å†å²æ‹‰å–èŒƒå›´: {start_date} åˆ° {end_date}")
        while current_date <= end:
            date_str = current_date.strftime('%Y-%m-%d')
            local_path = os.path.join('archive', 'daily', date_str[:4], f"{date_str}.md")
            logger.debug(f"æ£€æŸ¥æœ¬åœ°æ–‡ä»¶: {local_path}")
            doonsec_list = []
            chainreactors_urls = []
            brucefeiix_urls = []
            if os.path.exists(local_path):
                with open(local_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                urls = re.findall(r'(https://mp.weixin.qq.com/[\w\-\?&=%.]+)', content, re.I)
                urls = [url.rstrip(')') for url in urls]
                chainreactors_urls = urls
            process_one_day(date_str, doonsec_list, chainreactors_urls, brucefeiix_urls, data, data_file, base_result_path, executable_path)
            current_date += datetime.timedelta(days=1)
    elif args.date:
        logger.info(f"=== å¼€å§‹æŒ‡å®šæ—¥æœŸæ‹‰å–: {args.date} ===")
        date_str = args.date
        doonsec_list = get_doonsec_url()  # [(url, date, title)]
        # ChainReactors
        chainreactors_urls = []
        cr_md_url = get_chainreactors_md_url(date_str)
        logger.info(f"ChainReactors mdæ–‡ä»¶URL: {cr_md_url}")
        if cr_md_url:
            try:
                resp = requests.get(cr_md_url)
                logger.info(f"ChainReactors mdæ–‡ä»¶ä¸‹è½½çŠ¶æ€ç : {resp.status_code}")
                if resp.status_code == 200:
                    chainreactors_urls = parse_md_urls_with_title(resp.text)
                    logger.info(f"ChainReactorsè·å–åˆ° {len(chainreactors_urls)} ä¸ªURL")
                else:
                    logger.warning(f"ChainReactors mdæ–‡ä»¶ä¸‹è½½å¤±è´¥: {cr_md_url} çŠ¶æ€ç : {resp.status_code}")
            except Exception as e:
                logger.error(f"ChainReactors mdè§£æå¤±è´¥: {e}")
        else:
            logger.warning("ChainReactors mdæ–‡ä»¶URLä¸ºç©º")
        # BruceFeIix
        brucefeiix_urls = []
        bf_md_url = get_BruceFeIix_md_url(date_str)
        logger.info(f"BruceFeIix mdæ–‡ä»¶URL: {bf_md_url}")
        if bf_md_url:
            try:
                resp = requests.get(bf_md_url)
                logger.info(f"BruceFeIix mdæ–‡ä»¶ä¸‹è½½çŠ¶æ€ç : {resp.status_code}")
                if resp.status_code == 200:
                    brucefeiix_urls = parse_md_urls_with_title(resp.text)
                    logger.info(f"BruceFeIixè·å–åˆ° {len(brucefeiix_urls)} ä¸ªURL")
                else:
                    logger.warning(f"BruceFeIix mdæ–‡ä»¶ä¸‹è½½å¤±è´¥: {bf_md_url} çŠ¶æ€ç : {resp.status_code}")
            except Exception as e:
                logger.error(f"BruceFeIix mdè§£æå¤±è´¥: {e}")
        else:
            logger.warning("BruceFeIix mdæ–‡ä»¶URLä¸ºç©º")
        process_one_day(date_str, doonsec_list, chainreactors_urls, brucefeiix_urls, data, data_file, base_result_path, executable_path)
    elif args.range:
        logger.info(f"=== å¼€å§‹æ—¥æœŸåŒºé—´æ‹‰å–: {args.range[0]} åˆ° {args.range[1]} ===")
        start, end = args.range
        start_dt = datetime.datetime.strptime(start, "%Y-%m-%d")
        end_dt = datetime.datetime.strptime(end, "%Y-%m-%d")
        current_date = start_dt
        while current_date <= end_dt:
            date_str = current_date.strftime('%Y-%m-%d')
            doonsec_list = get_doonsec_url()
            # ChainReactors
            chainreactors_urls = []
            cr_md_url = get_chainreactors_md_url(date_str)
            logger.info(f"ChainReactors mdæ–‡ä»¶URL: {cr_md_url}")
            if cr_md_url:
                try:
                    resp = requests.get(cr_md_url)
                    logger.info(f"ChainReactors mdæ–‡ä»¶ä¸‹è½½çŠ¶æ€ç : {resp.status_code}")
                    if resp.status_code == 200:
                        chainreactors_urls = parse_md_urls_with_title(resp.text)
                        logger.info(f"ChainReactorsè·å–åˆ° {len(chainreactors_urls)} ä¸ªURL")
                    else:
                        logger.warning(f"ChainReactors mdæ–‡ä»¶ä¸‹è½½å¤±è´¥: {cr_md_url} çŠ¶æ€ç : {resp.status_code}")
                except Exception as e:
                    logger.error(f"ChainReactors mdè§£æå¤±è´¥: {e}")
            else:
                logger.warning("ChainReactors mdæ–‡ä»¶URLä¸ºç©º")
            # BruceFeIix
            brucefeiix_urls = []
            bf_md_url = get_BruceFeIix_md_url(date_str)
            logger.info(f"BruceFeIix mdæ–‡ä»¶URL: {bf_md_url}")
            if bf_md_url:
                try:
                    resp = requests.get(bf_md_url)
                    logger.info(f"BruceFeIix mdæ–‡ä»¶ä¸‹è½½çŠ¶æ€ç : {resp.status_code}")
                    if resp.status_code == 200:
                        brucefeiix_urls = parse_md_urls_with_title(resp.text)
                        logger.info(f"BruceFeIixè·å–åˆ° {len(brucefeiix_urls)} ä¸ªURL")
                    else:
                        logger.warning(f"BruceFeIix mdæ–‡ä»¶ä¸‹è½½å¤±è´¥: {bf_md_url} çŠ¶æ€ç : {resp.status_code}")
                except Exception as e:
                    logger.error(f"BruceFeIix mdè§£æå¤±è´¥: {e}")
            else:
                logger.warning("BruceFeIix mdæ–‡ä»¶URLä¸ºç©º")
            process_one_day(date_str, doonsec_list, chainreactors_urls, brucefeiix_urls, data, data_file, base_result_path, executable_path)
            current_date += datetime.timedelta(days=1)
    else:
        logger.info("=== å¼€å§‹ä»Šæ—¥æ‹‰å– ===")
        current_date = datetime.datetime.now()
        date_str = current_date.strftime('%Y-%m-%d')
        doonsec_list = get_doonsec_url()
        # ChainReactors
        chainreactors_urls = []
        cr_md_url = get_chainreactors_md_url(date_str)
        logger.info(f"ChainReactors mdæ–‡ä»¶URL: {cr_md_url}")
        if cr_md_url:
            try:
                resp = requests.get(cr_md_url)
                logger.info(f"ChainReactors mdæ–‡ä»¶ä¸‹è½½çŠ¶æ€ç : {resp.status_code}")
                if resp.status_code == 200:
                    chainreactors_urls = parse_md_urls_with_title(resp.text)
                    logger.info(f"ChainReactorsè·å–åˆ° {len(chainreactors_urls)} ä¸ªURL")
                else:
                    logger.warning(f"ChainReactors mdæ–‡ä»¶ä¸‹è½½å¤±è´¥: {cr_md_url} çŠ¶æ€ç : {resp.status_code}")
            except Exception as e:
                logger.error(f"ChainReactors mdè§£æå¤±è´¥: {e}")
        else:
            logger.warning("ChainReactors mdæ–‡ä»¶URLä¸ºç©º")
        # BruceFeIix
        brucefeiix_urls = []
        bf_md_url = get_BruceFeIix_md_url(date_str)
        logger.info(f"BruceFeIix mdæ–‡ä»¶URL: {bf_md_url}")
        if bf_md_url:
            try:
                resp = requests.get(bf_md_url)
                logger.info(f"BruceFeIix mdæ–‡ä»¶ä¸‹è½½çŠ¶æ€ç : {resp.status_code}")
                if resp.status_code == 200:
                    brucefeiix_urls = parse_md_urls_with_title(resp.text)
                    logger.info(f"BruceFeIixè·å–åˆ° {len(brucefeiix_urls)} ä¸ªURL")
                else:
                    logger.warning(f"BruceFeIix mdæ–‡ä»¶ä¸‹è½½å¤±è´¥: {bf_md_url} çŠ¶æ€ç : {resp.status_code}")
            except Exception as e:
                logger.error(f"BruceFeIix mdè§£æå¤±è´¥: {e}")
        else:
            logger.warning("BruceFeIix mdæ–‡ä»¶URLä¸ºç©º")
        process_one_day(date_str, doonsec_list, chainreactors_urls, brucefeiix_urls, data, data_file, base_result_path, executable_path)
    logger.info("=== æ‰§è¡Œå®Œæˆ ===")

if __name__ == '__main__':
    main()
